{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling TCGA data\n",
    "## Your input\n",
    "- Filter and select TCGA data from the TCGA GDC data portal as explained in [\"TCGA_steps_explained.ipynb\"](TCGA_steps_explained.ipynb)\n",
    "- Follow these steps every time for your new analyses, also when you have new aspects or file types to consider later on\n",
    "- Create a folder called \"sample_sheets\" in your analysis path, create the folders \"manifests\" and \"sample_sheets_prior\" in the \"sample_sheets\" folder\n",
    "- Adapt the [\"data/config.yaml\"](data/config.yaml)\n",
    "\n",
    "## Check validity of configuration file entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check validity of important configuration file entries\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# Check analysis path\n",
    "analysis_path = config_file['analysis_path']\n",
    "# if analysis_path is valid\n",
    "if not os.path.exists(analysis_path):\n",
    "    print('Please make sure the analysis path exists and includes the manifest files. Your current analysis path is the following:')\n",
    "    print(analysis_path)\n",
    "else:\n",
    "    # if analysis_path has \"/\" at the end\n",
    "    if analysis_path[-1] != '/':\n",
    "        analysis_path = analysis_path+'/'\n",
    "        change_config_cmd = f\"sed -i '/^analysis_path: .*/s/$/\\//' data/config.yaml\"\n",
    "        subprocess.run(shlex.split(change_config_cmd))\n",
    "    # if sample sheet and manifests folders exist\n",
    "    if not os.path.exists(analysis_path+'sample_sheets/manifests') or not os.path.exists(analysis_path+'sample_sheets/sample_sheets_prior'):\n",
    "        print('Please make sure the paths sample_sheets/manifests and sample_sheets/sample_sheets_prior exist in your analysis folder.')\n",
    "    else:\n",
    "        # if files in manifest and sample sheet folders exist\n",
    "        manifests_prior = config_file['manifests_prior']\n",
    "        sample_sheets_prior = config_file['sample_sheets_prior']\n",
    "\n",
    "        for category_list, category_name in zip([manifests_prior, sample_sheets_prior], ['manifests', 'sample_sheets_prior']):\n",
    "            for check_file in category_list:\n",
    "                if not os.path.isfile(analysis_path+f'sample_sheets/{category_name}/'+check_file):\n",
    "                    print(f'{check_file} ist not existent. Please check again.')\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        # if sample sheet for filtering is existent\n",
    "        sample_sheet_filtering = config_file['sample_sheet_filtering']\n",
    "\n",
    "        if sample_sheet_filtering != False:\n",
    "            if not os.path.isfile(sample_sheet_filtering):\n",
    "                print(f'{sample_sheet_filtering} is not existent. Please check again or input: False')\n",
    "        \n",
    "        # if manifest for download is existent\n",
    "        manifest_for_download = config_file['manifest_for_download']\n",
    "\n",
    "        if manifest_for_download != False:\n",
    "            for check_file in manifest_for_download:\n",
    "                if not os.path.isfile(analysis_path+f'sample_sheets/manifests/'+check_file):\n",
    "                    print(f'{check_file} is not existent. Please check again or input: False')\n",
    "\n",
    "# Check if TCGA user token file is existent\n",
    "tcga_user_token_file = config_file['tcga_user_token_file']\n",
    "if tcga_user_token_file != False:\n",
    "    if not os.path.isfile(tcga_user_token_file):\n",
    "        print(f'User token file not existent, please check again or if you do not want to use one, input: False. Your current analysis path is the following:')\n",
    "        print(tcga_user_token_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine manifest with sample sheet, filter for relevant files\n",
    "\n",
    "- Merge manifest and sample sheet\n",
    "- If previous selection of case IDs -> filter for specific case IDs of previous analysis\n",
    "- Create adapted filtered manifest files for gdc-client download, rename them with the suffix \"_ready.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# load config file\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# analysis path\n",
    "analysis_path = config_file['analysis_path']\n",
    "\n",
    "# original (prior) manifest files, merge them\n",
    "manifests = config_file['manifests_prior']\n",
    "manifests_dfs = [pd.read_table(analysis_path+'sample_sheets/manifests/'+i) for i in manifests]\n",
    "\n",
    "# original (prior) sample sheets, merge them\n",
    "sample_sheets = [pd.read_table(analysis_path+'sample_sheets/sample_sheets_prior/'+i) for i in config_file['sample_sheets_prior']]\n",
    "sample_sheets_merge = pd.concat(sample_sheets)\n",
    "\n",
    "sample_sheets_merge['Case ID'] = sample_sheets_merge['Case ID'].str.split(', ', expand=True)[0]\n",
    "\n",
    "# if filtering of manifest file on case IDs of previous sample sheet is wanted\n",
    "sample_sheet_filtering = config_file['sample_sheet_filtering']\n",
    "\n",
    "if sample_sheet_filtering != False:\n",
    "    sample_sheet_filter = pd.read_table(analysis_path+'sample_sheets/'+sample_sheet_filtering)\n",
    "    case_ids = list(sample_sheet_filter['Case ID'].unique())\n",
    "    for manifest, file_name_manifest in zip(manifests_dfs, manifests):\n",
    "        filtered_manifest = manifest[(manifest['Case ID'].isin(case_ids))&(manifest['Sample Type']=='Primary Tumor')].copy()\n",
    "        filtered_manifest.to_csv((analysis_path+'sample_sheets/manifests/'+manifest[0]+'_ready'+'.txt'), sep='\\t', index=False)\n",
    "else:\n",
    "    for manifest, file_name_manifest in zip(manifests_dfs, manifests):\n",
    "        manifest.to_csv((analysis_path+'sample_sheets/manifests/'+manifest[0]+'_ready'+'.txt'), sep='\\t', index=False)\n",
    "\n",
    "\n",
    "# change config.yaml \"manifest_for_download\" with manifests file name rsplit('.',1)[0] + _ready + manifests file name rsplit('.',1)[0]\n",
    "\n",
    "# manifests for download\n",
    "import shlex\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# filtered manifest(s) in there, but only names in 'sample_sheets/manifests/\n",
    "if config_file['manifest_for_download'] == False:\n",
    "    manifests_pipeline = ', '.join(manifests_pipeline_files) # only file names\n",
    "    \n",
    "    manifest_file_change_cmd = f\"sed -i 's/^manifest_for_download: .*/manifest_for_download: [{manifests_pipeline}]/' data/config.yaml\"\n",
    "    subprocess.run(shlex.split(manifest_file_change_cmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TCGA data via a manifest document and the GDC-client tool\n",
    "- This part creates a new conda environment called \"gdc_client\" and downloads the gdc-client tool\n",
    "- Download of the TCGA data from manifest specified in previous steps and/or the configuration file via the gdc-client\n",
    "- The files from the manifest are downloaded into the following folder: analysis_path + '00_raw_data'\n",
    "\n",
    "- Optional: for restricted access files:\n",
    "    - Login at NIH for restricted access files\n",
    "    - Download access token, save as secured file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "\n",
    "```\n",
    "gdc-client download -m manifest.txt -t user-token.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import shlex\n",
    "\n",
    "# Read in manifest download files (either from manual input in config.yaml or from previous pipeline steps)\n",
    "def Create_Manifest_Download_List():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    manifest_download_list = config_file['manifest_for_download']\n",
    "    analysis_path = config_file['analysis_path']\n",
    "\n",
    "    if manifest_download_list == False:\n",
    "        manifest_for_download = False\n",
    "        print('Please execute the previous part of the pipeline or input your files manually in the data/config.yaml file')\n",
    "    else:\n",
    "        manifest_for_download = [analysis_path+'sample_sheets/manifests/'+i for i in manifest_download_list]\n",
    "    \n",
    "    return manifest_for_download\n",
    "\n",
    "\n",
    "# Check whether files in manifest have already been downloaded an exclude them from the manifest\n",
    "def Check_Already_Downloaded():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    analysis_path = config_file['analysis_path']\n",
    "    \n",
    "    manifest_for_download_prior = Create_Manifest_Download_List()\n",
    "\n",
    "    manifest_for_download = []\n",
    "\n",
    "    if manifest_for_download_prior != False:\n",
    "        for manifest_file in manifest_for_download_prior:\n",
    "            manifest = pd.read_table(manifest_file).drop_duplicates()\n",
    "            manifest['file_path'] = analysis_path + '00_raw_data/' + manifest['id'] + '/' + manifest['filename']\n",
    "            manifest['is_file'] = manifest['file_path'].apply(os.path.isfile)\n",
    "            manifest2 = manifest[manifest['is_file']==False].copy()\n",
    "            manifest_file_name_2 = manifest_file.rsplit('.', 1)[0] + '_check' + manifest_file.rsplit('.', 1)[1]\n",
    "            manifest2.to_csv(manifest_file_name_2)\n",
    "            manifest_for_download.append(manifest_file_name_2)\n",
    "    \n",
    "    return manifest_for_download\n",
    "\n",
    "\n",
    "# Download the gdc-client in a new conda environment and run the gdc-client, if accepted\n",
    "def Download_gdc_client():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    conda_gdc = config_file['conda_gdc']\n",
    "    name_conda_gdc_env = False\n",
    "\n",
    "    if conda_gdc == False:\n",
    "        print('Please execute the TCGA data download in your own environment or '+\n",
    "            'set \"conda_gdc\" in the data/config.yaml file to \"True\" to create a conda environment with the gdc-client')\n",
    "    elif conda_gdc == True:\n",
    "        name_conda_gdc_env = 'gdc_client'\n",
    "    elif conda_gdc == 'First_install':\n",
    "        gdc_client_conda_cmd = shlex.split(f'conda create --name gdc_client --file envs/gdc_client.txt')\n",
    "        subprocess.run(gdc_client_conda_cmd)\n",
    "        \n",
    "        conda_gdc_status_change_cmd = f\"sed -i 's/^conda_gdc: .*/conda_gdc: True/' data/config.yaml\"\n",
    "        subprocess.run(shlex.split(conda_gdc_status_change_cmd))\n",
    "\n",
    "        name_conda_gdc_env = 'gdc_client'\n",
    "    else:\n",
    "        name_conda_gdc_env = conda_gdc\n",
    "    \n",
    "    return name_conda_gdc_env\n",
    "\n",
    "\n",
    "# Prepare commands to download TCGA data, dependent on available user token file\n",
    "def TCGA_Data_Download():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    tcga_user_token_file = config_file['tcga_user_token_file']\n",
    "    analysis_path = config_file['analysis_path']\n",
    "    raw_data_path = analysis_path + '00_raw_data'\n",
    "\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "\n",
    "    manifest_for_download = Check_Already_Downloaded()\n",
    "\n",
    "    name_conda_gdc_env = Download_gdc_client()\n",
    "\n",
    "    if manifest_for_download == False:\n",
    "        print('No TCGA data are download due to no manifest files.')\n",
    "    elif name_conda_gdc_env == False:\n",
    "        print('No TCGA data are downloaded due to the specifications in the gdc client conda environment.')\n",
    "    else:\n",
    "        for manifest_file in manifest_for_download:\n",
    "            if tcga_user_token_file == False:\n",
    "                print(f'Download TCGA data with TCGA manifest {manifest_file.split(\"/\")[-1]} without TCGA user token')\n",
    "                command_download_tcga_data = f'conda run -n {name_conda_gdc_env} gdc-client download -m {manifest_file}'\n",
    "            else:\n",
    "                print(f'Download TCGA data with TCGA manifest {manifest_file.split(\"/\")[-1]} with TCGA user token file {tcga_user_token_file.split(\"/\")[-1]}')\n",
    "                command_download_tcga_data = f'conda run -n {name_conda_gdc_env} gdc-client download -m {manifest_file} {tcga_user_token_file}'\n",
    "            process = subprocess.Popen(command_download_tcga_data, cwd=raw_data_path, shell=True)\n",
    "            process.wait()\n",
    "\n",
    "TCGA_Data_Download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename file names with symlinks\n",
    "- In manifest only id, filename with 36 different characters\n",
    "- Merge manifest and sample sheet, create new names \n",
    "- Rename downloaded files and put them in new folders for each analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "analysis_path = config_file['analysis_path']\n",
    "os.makedirs(analysis_path+'01_sample_data', exist_ok=True)\n",
    "\n",
    "downloaded_manifests = config_file['manifest_for_download']\n",
    "downloaded_manifests = pd.concat([pd.read_table(analysis_path+'sample_sheets/manifests/'+i.rsplit('.', 1)[0]+'_ready'+i.rsplit('.', 1)[1]) for i in downloaded_manifests])\n",
    "downloaded_manifests = downloaded_manifests.rename(columns={'id':'File ID', 'filename':'File Name'})\n",
    "\n",
    "# original (prior) sample sheets, merge them\n",
    "sample_sheets = [pd.read_table(analysis_path+'sample_sheets/sample_sheets_prior/'+i) for i in config_file['sample_sheets_prior']]\n",
    "sample_sheets_merge = pd.concat(sample_sheets)\n",
    "\n",
    "sample_sheets_merge['Case ID'] = sample_sheets_merge['Case ID'].str.split(', ', expand=True)[0]\n",
    "\n",
    "# categorize samples for their analysis method\n",
    "method_dict = {'BRASS':'BRASS', 'CaVEMan':'CaVEMan', 'ASCAT':'CNV_segment', 'pindel':'Pindel', 'star_splice':'Splicing', \n",
    "               'star_gene_counts':'STAR_counts'}\n",
    "\n",
    "sample_sheets_merge['Folder'] = ''\n",
    "\n",
    "for met in method_dict.keys():\n",
    "    sample_sheets_merge.loc[sample_sheets_merge['File Name'].str.contains(met), 'Folder'] = method_dict[met]\n",
    "\n",
    "# 36 characters file ID\n",
    "sample_sheets_merge['File Suffix'] = sample_sheets_merge['File Name'].str[36:]\n",
    "\n",
    "sample_sheets_merge['Path_raw'] = analysis_path+'00_raw_data/'+sample_sheets_merge['File ID']+'/'+sample_sheets_merge['File Name']\n",
    "sample_sheets_merge['Path_sample'] = analysis_path+'01_sample_data/'+sample_sheets_merge['Folder']+'/'+sample_sheets_merge['Case ID']+sample_sheets_merge['File Suffix']\n",
    "\n",
    "# merge with executed manifest file(s) on executed manifest files\n",
    "downloaded_manifests = downloaded_manifests.merge(sample_sheets_merge, how='left', on=['File ID', 'File Name'])\n",
    "\n",
    "# create folders for methods\n",
    "methods = list(downloaded_manifests['Folder'].unique())\n",
    "for m in methods:\n",
    "    os.makedirs(analysis_path+'01_sample_data/'+m, exist_ok=True)\n",
    "\n",
    "# create symlinks for samples to categorize them in \n",
    "for raw_path, sample_path in zip(list(downloaded_manifests['Path_raw']), list(downloaded_manifests['Path_sample'])):\n",
    "    try:\n",
    "        os.symlink(raw_path, sample_path)\n",
    "    except FileExistsError:\n",
    "        print(sample_path.split('/')[-1] + ' already exists.')\n",
    "    except FileNotFoundError:\n",
    "        print(sample_path.split('/')[-1] + ' not found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze files\n",
    "- with Snakemake pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "import shlex\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "conda_snakemake = config_file['conda_snakemake']\n",
    "snakemake_threads = config_file['snakemake_threads']\n",
    "\n",
    "# run Snakemake pipeline\n",
    "command_snakemake = f'conda run -n {conda_snakemake} snakemake --codes {snakemake_threads} --use-conda'\n",
    "process_snakemake = subprocess.Popen(command_snakemake, shell=True)\n",
    "process_snakemake.wait()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Plots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
