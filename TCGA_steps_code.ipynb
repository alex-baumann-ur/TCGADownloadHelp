{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling TCGA data\n",
    "## Your input\n",
    "- Filter and select TCGA data from the TCGA GDC data portal as explained in [\"TCGA_steps_explained.ipynb\"](TCGA_steps_explained.ipynb)\n",
    "- Follow these steps every time for your new analyses, also when you have new aspects or file types to consider later on\n",
    "- Create a folder called \"sample_sheets\" in your analysis path, create the folders \"manifests\" and \"sample_sheets_prior\" in the \"sample_sheets\" folder\n",
    "- Adapt the [\"data/config.yaml\"](data/config.yaml)\n",
    "\n",
    "## Check validity of configuration file entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check validity of important configuration file entries\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# Check analysis path\n",
    "analysis_path = config_file['analysis_path']\n",
    "# if analysis_path is valid\n",
    "if not os.path.exists(analysis_path):\n",
    "    print('Please make sure the analysis path exists and includes the manifest files. Your current analysis path is the following:')\n",
    "    print(analysis_path)\n",
    "else:\n",
    "    # if analysis_path has \"/\" at the end\n",
    "    if analysis_path[-1] != '/':\n",
    "        analysis_path = analysis_path+'/'\n",
    "        change_config_cmd = f\"sed -i '/^analysis_path: .*/s/$/\\//' data/config.yaml\"\n",
    "        subprocess.run(shlex.split(change_config_cmd))\n",
    "    # if sample sheet and manifests folders exist\n",
    "    if not os.path.exists(analysis_path+'sample_sheets/manifests') or not os.path.exists(analysis_path+'sample_sheets/sample_sheets_prior'):\n",
    "        print('Please make sure the paths sample_sheets/manifests and sample_sheets/sample_sheets_prior exist in your analysis folder.')\n",
    "    else:\n",
    "        # if files in manifest and sample sheet folders exist\n",
    "        manifests_prior = config_file['manifests_prior']\n",
    "        sample_sheets_prior = config_file['sample_sheets_prior']\n",
    "\n",
    "        for category_list, category_name in zip([manifests_prior, sample_sheets_prior], ['manifests', 'sample_sheets_prior']):\n",
    "            for check_file in category_list:\n",
    "                if not os.path.isfile(analysis_path+f'sample_sheets/{category_name}/'+check_file):\n",
    "                    print(f'{check_file} ist not existent. Please check again.')\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        # if sample sheet for filtering is existent\n",
    "        sample_sheet_filtering = config_file['sample_sheet_filtering']\n",
    "\n",
    "        if sample_sheet_filtering != False:\n",
    "            if not os.path.isfile(sample_sheet_filtering):\n",
    "                print(f'{sample_sheet_filtering} is not existent. Please check again or input: False')\n",
    "        \n",
    "        # if manifest for download is existent\n",
    "        manifest_for_download = config_file['manifest_for_download']\n",
    "\n",
    "        if manifest_for_download != False:\n",
    "            for check_file in manifest_for_download:\n",
    "                if not os.path.isfile(analysis_path+f'sample_sheets/manifests/'+check_file):\n",
    "                    print(f'{check_file} is not existent. Please check again or input: False')\n",
    "\n",
    "# Check if TCGA user token file is existent\n",
    "tcga_user_token_file = config_file['tcga_user_token_file']\n",
    "if tcga_user_token_file != False:\n",
    "    if not os.path.isfile(tcga_user_token_file):\n",
    "        print(f'User token file not existent, please check again or if you do not want to use one, input: False. Your current analysis path is the following:')\n",
    "        print(tcga_user_token_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine manifest with sample sheet, filter for relevant files to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import shlex\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# load config file\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# analysis path\n",
    "analysis_path = config_file['analysis_path']\n",
    "\n",
    "# original (prior) manifest files\n",
    "manifests = config_file['manifests_prior']\n",
    "manifests_dfs = [pd.read_table(analysis_path+'sample_sheets/manifests/'+i) for i in manifests]\n",
    "\n",
    "# create folder for downloadable manifest files and merged with sample sheet\n",
    "os.makedirs(analysis_path+'sample_sheets/manifests_for_download', exist_ok=True)\n",
    "os.makedirs(analysis_path+'sample_sheets/manifests_merged_sample_sheet', exist_ok=True)\n",
    "\n",
    "# original (prior) sample sheets, merge them\n",
    "sample_sheets = [pd.read_table(analysis_path+'sample_sheets/sample_sheets_prior/'+i) for i in config_file['sample_sheets_prior']]\n",
    "sample_sheets_merge = pd.concat(sample_sheets)\n",
    "\n",
    "sample_sheets_merge['Case ID'] = sample_sheets_merge['Case ID'].str.split(', ', expand=True)[0]\n",
    "\n",
    "# merge each manifest file with the sample sheet\n",
    "merge_manifest_sample_sheet = []\n",
    "for manifest in manifests_dfs:\n",
    "    df = pd.merge(manifest, sample_sheets_merge, how='left', left_on=['id','filename'], right_on=['File ID','File Name'], indicator=True)\n",
    "\n",
    "    \n",
    "    if len(df[df['_merge']=='left_only'])>0:\n",
    "        print('It seems like a manifest entry cannot be mapped to the sample sheet. Please check your manifest file and sample sheet again.')\n",
    "        print('Only mapped samples are further processed.')\n",
    "        df = df[df['_merge']=='both'].copy()\n",
    "    merge_manifest_sample_sheet.append(df.drop(columns='_merge'))\n",
    "\n",
    "\n",
    "# Check whether files in manifest have already been downloaded an exclude them from the manifest\n",
    "def Check_Already_Downloaded(manifest_sample_sheet_df, method_dict):\n",
    "    df = manifest_sample_sheet_df.copy().drop_duplicates()\n",
    "    \n",
    "    df['Folder'] = ''\n",
    "    for met in method_dict.keys():\n",
    "        df.loc[df['File Name'].str.contains(met), 'Folder'] = method_dict[met]\n",
    "    \n",
    "    # 36 characters file ID\n",
    "    df['File Suffix'] = df['File Name'].str[36:]\n",
    "    df['Path_raw'] = analysis_path+'00_raw_data/'+df['File ID']+'/'+df['File Name']\n",
    "    df['Path_sample'] = analysis_path+'01_sample_data/'+df['Folder']+'/'+df['Case ID']+df['File Suffix']\n",
    "\n",
    "    # check if file was already downloaded\n",
    "    df['is_file_raw'] = df['Path_raw'].apply(os.path.isfile)\n",
    "    df['is_file_caseid'] = df['Path_sample'].apply(os.path.isfile)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# if filtering of manifest file on case IDs of previous sample sheet is wanted\n",
    "# include step of checking whether files are already downloaded\n",
    "sample_sheet_filtering = config_file['sample_sheet_filtering']\n",
    "method_dict = config_file['methods_dict']\n",
    "\n",
    "if sample_sheet_filtering != False:\n",
    "    sample_sheet_filter = pd.read_table(analysis_path+'sample_sheets/'+sample_sheet_filtering)\n",
    "    case_ids = list(sample_sheet_filter['Case ID'].unique())\n",
    "    list_manifests_for_download = []\n",
    "    for manifest, file_name_manifest in zip(merge_manifest_sample_sheet, manifests):\n",
    "        # only include primary tumor samples\n",
    "        filtered_manifest = manifest[(manifest['Case ID'].isin(case_ids))&(manifest['Sample Type']=='Primary Tumor')].copy()\n",
    "        \n",
    "        # check for already downloaded files\n",
    "        filtered_manifest = Check_Already_Downloaded(filtered_manifest, method_dict)\n",
    "\n",
    "        # save filtered and merged manifest with old and new file names\n",
    "        filtered_manifest.to_csv(analysis_path+'sample_sheets/manifests_merged_sample_sheet/'+file_name_manifest.rsplit('.', 1)[0]+'_merged_sample_sheet.txt', \n",
    "                                 sep='\\t', index=False)\n",
    "\n",
    "        # only include not already downloaded entries\n",
    "        filtered_checked_manifest = filtered_manifest[(filtered_manifest['is_file_raw']==False)&(filtered_manifest['is_file_caseid']==False)]\n",
    "        filtered_manifest_name = analysis_path+'sample_sheets/manifests_for_download/'+file_name_manifest.rsplit('.', 1)[0]+'_ready.txt'\n",
    "        list_manifests_for_download.append(filtered_manifest_name)\n",
    "        # save filtered manifest for download\n",
    "        filtered_checked_manifest[['id','filename','md5','size','state']].to_csv(filtered_manifest_name, sep='\\t', index=False)\n",
    "            \n",
    "else:\n",
    "    list_manifests_for_download = []\n",
    "    for manifest, file_name_manifest in zip(merge_manifest_sample_sheet, manifests):\n",
    "        # check for already downloaded files\n",
    "        manifest = Check_Already_Downloaded(manifest, method_dict)\n",
    "\n",
    "        # save merged manifest with old and new file names\n",
    "        manifest.to_csv(analysis_path+'sample_sheets/manifests_merged_sample_sheet/'+file_name_manifest.rsplit('.', 1)[0]+'_merged_sample_sheet.txt', sep='\\t', index=False)\n",
    "\n",
    "        # only include not already downloaded entries\n",
    "        checked_manifest = manifest[(manifest['is_file_raw']==False)&(manifest['is_file_caseid']==False)]\n",
    "        manifest_name = analysis_path+'sample_sheets/manifests_for_download/'+file_name_manifest.rsplit('.', 1)[0]+'_ready.txt'\n",
    "        list_manifests_for_download.append(manifest_name)\n",
    "        # save manifest for download\n",
    "        checked_manifest[['id','filename','md5','size','state']].to_csv(manifest_name, sep='\\t', index=False)\n",
    "        \n",
    "\n",
    "# manifests for download\n",
    "# read in configuration file again because it was changed the prior step\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# filtered manifest(s) in there, but only names in 'sample_sheets/manifests/\n",
    "if config_file['manifest_for_download'] == False:\n",
    "    manifests_pipeline_files = [m.split('/')[-1] for m in list_manifests_for_download]\n",
    "    manifests_pipeline = ', '.join(manifests_pipeline_files) # only file names\n",
    "    \n",
    "    manifest_file_change_cmd = f\"sed -i 's/^manifest_for_download: .*/manifest_for_download: [{manifests_pipeline}]/' data/config.yaml\"\n",
    "    subprocess.run(shlex.split(manifest_file_change_cmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TCGA data via a manifest document and the GDC-client tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import shlex\n",
    "\n",
    "# Read in manifest download files (either from manual input in config.yaml or from previous pipeline steps)\n",
    "def Create_Manifest_Download_List():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    manifest_download_list = config_file['manifest_for_download']\n",
    "    analysis_path = config_file['analysis_path']\n",
    "\n",
    "    if manifest_download_list == False:\n",
    "        manifest_for_download = False\n",
    "        # merge_manifest_sample_sheet = False\n",
    "        print('Please execute the previous part of the pipeline or input your files manually in the data/config.yaml file')\n",
    "    else:\n",
    "        manifest_for_download = [analysis_path+'sample_sheets/manifests_for_download/'+i for i in manifest_download_list]\n",
    "        # merge_manifest_sample_sheet = []\n",
    "    \n",
    "    return manifest_for_download\n",
    "\n",
    "\n",
    "# Download the gdc-client in a new conda environment and run the gdc-client, if accepted\n",
    "def Download_gdc_client():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    conda_gdc = config_file['conda_gdc']\n",
    "    name_conda_gdc_env = False\n",
    "\n",
    "    if conda_gdc == False:\n",
    "        print('Please execute the TCGA data download in your own environment or '+\n",
    "            'set \"conda_gdc\" in the data/config.yaml file to \"First_install\" to create a conda environment with the gdc-client')\n",
    "    elif conda_gdc == True:\n",
    "        name_conda_gdc_env = 'gdc_client'\n",
    "    elif conda_gdc == 'First_install':\n",
    "        gdc_client_conda_cmd = shlex.split(f'conda create --name gdc_client --file envs/gdc_client.txt')\n",
    "        subprocess.run(gdc_client_conda_cmd)\n",
    "        \n",
    "        conda_gdc_status_change_cmd = f\"sed -i 's/^conda_gdc: .*/conda_gdc: True/' data/config.yaml\"\n",
    "        subprocess.run(shlex.split(conda_gdc_status_change_cmd))\n",
    "\n",
    "        name_conda_gdc_env = 'gdc_client'\n",
    "    else:\n",
    "        name_conda_gdc_env = conda_gdc\n",
    "    \n",
    "    return name_conda_gdc_env\n",
    "\n",
    "\n",
    "# Prepare commands to download TCGA data, dependent on available user token file\n",
    "def TCGA_Data_Download():\n",
    "    with open('data/config.yaml', 'r') as streamfile:\n",
    "        config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "    \n",
    "    tcga_user_token_file = config_file['tcga_user_token_file']\n",
    "    analysis_path = config_file['analysis_path']\n",
    "    raw_data_path = analysis_path + '00_raw_data'\n",
    "\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "\n",
    "    manifest_for_download = Create_Manifest_Download_List()\n",
    "\n",
    "    name_conda_gdc_env = Download_gdc_client()\n",
    "\n",
    "    if manifest_for_download == False:\n",
    "        print('No TCGA data are download due to no manifest files.')\n",
    "    elif name_conda_gdc_env == False:\n",
    "        print('No TCGA data are downloaded due to the specifications in the gdc client conda environment.')\n",
    "    else:\n",
    "        for manifest_file in manifest_for_download:\n",
    "            if tcga_user_token_file == False:\n",
    "                print(f'Download TCGA data with TCGA manifest {manifest_file.split(\"/\")[-1]} without TCGA user token')\n",
    "                command_download_tcga_data = f'conda run -n {name_conda_gdc_env} gdc-client download -m {manifest_file}'\n",
    "            else:\n",
    "                print(f'Download TCGA data with TCGA manifest {manifest_file.split(\"/\")[-1]} with TCGA user token file {tcga_user_token_file.split(\"/\")[-1]}')\n",
    "                command_download_tcga_data = f'conda run -n {name_conda_gdc_env} gdc-client download -m {manifest_file} -t {tcga_user_token_file}'\n",
    "            process = subprocess.Popen(command_download_tcga_data, cwd=raw_data_path, shell=True)\n",
    "            process.wait()\n",
    "\n",
    "TCGA_Data_Download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename file names with symlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "analysis_path = config_file['analysis_path']\n",
    "os.makedirs(analysis_path+'01_sample_data', exist_ok=True)\n",
    "\n",
    "### work with already existing merged files in folder 'manifests_merged_sample_sheet'\n",
    "downloaded_manifests = config_file['manifest_for_download']\n",
    "downloaded_manifests = pd.concat([pd.read_table(analysis_path+'sample_sheets/manifests_merged_sample_sheet/'+i.rsplit('_ready.',1)[0]+'_merged_sample_sheet.txt') \n",
    "                                  for i in downloaded_manifests])\n",
    "\n",
    "# create folders for methods\n",
    "methods = list(downloaded_manifests['Folder'].unique())\n",
    "for m in methods:\n",
    "    os.makedirs(analysis_path+'01_sample_data/'+m, exist_ok=True)\n",
    "\n",
    "# create symlinks for samples to sort them into method folders\n",
    "list_rename_successful = []\n",
    "for raw_path, sample_path in zip(list(downloaded_manifests['Path_raw']), list(downloaded_manifests['Path_sample'])):\n",
    "    try:\n",
    "        os.symlink(raw_path, sample_path)\n",
    "        list_rename_successful.append('Success')\n",
    "    except FileExistsError:\n",
    "        list_rename_successful.append('Already exists')\n",
    "    except FileNotFoundError:\n",
    "        list_rename_successful.append('Not found')\n",
    "\n",
    "downloaded_manifests['rename_success'] = list_rename_successful\n",
    "\n",
    "downloaded_manifests[['Path_raw','Path_sample','rename_success']].to_csv((analysis_path+'01_sample_data/'+datetime.now().strftime('%Y-%m-%d_%H-%M-%S')+'_rename_files.txt'), \n",
    "                                                                         sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze files with Snakemake pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "with open('data/config.yaml', 'r') as streamfile:\n",
    "    config_file = yaml.load(streamfile, Loader=yaml.FullLoader)\n",
    "\n",
    "conda_snakemake = config_file['conda_snakemake']\n",
    "snakemake_threads = config_file['snakemake_threads']\n",
    "\n",
    "# run Snakemake pipeline\n",
    "command_snakemake = f'conda run -n {conda_snakemake} snakemake --cores {snakemake_threads} --use-conda -k'\n",
    "process_snakemake = subprocess.Popen(command_snakemake, shell=True)\n",
    "process_snakemake.wait()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Plots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
